"""
å¤æ‚çš„å¤šæº iPhone ä»·æ ¼æŠ“å–å™¨ï¼Œæ”¯æŒè‡ªå®šä¹‰ç½‘é¡µå’Œ API æºï¼Œ
å¤šæ ¼å¼è§£æï¼Œå­˜å…¥ SQLite æ•°æ®åº“ï¼Œå…·æœ‰æ—¥å¿—ã€æ ¡éªŒã€æ ‡å‡†åŒ–å¤„ç†ç­‰åŠŸèƒ½ã€‚
"""

import requests
import sqlite3
import logging
import random
import time
import re
from datetime import datetime
from typing import List, Dict, Optional, Union
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import threading
import json

# ---------------- é…ç½® ----------------
DB_PATH = "data/iphone_prices.db"
LOG_FILE = "logs/get_prices.log"

MAX_RETRIES = 3
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)",
    "Mozilla/5.0 (X11; Linux x86_64)"
]

DEFAULT_HEADERS = {
    "Accept-Language": "en-US,en;q=0.9"
}

URLS = [
    "https://example.com/iphone14-pro-japan",
    "https://example.com/iphone14-pro-global",
    "https://example.com/api/price-feed?model=iphone13",
    "https://example.com/deals/iphone15"
]

# ---------------- æ—¥å¿— ----------------
logging.basicConfig(filename=LOG_FILE,
                    format="%(asctime)s - %(levelname)s - %(message)s",
                    level=logging.INFO)

# ---------------- æŠ“å–å™¨ ----------------
class PageFetcher:
    def __init__(self, url: str):
        self.url = url
        self.session = requests.Session()

    def _headers(self):
        headers = DEFAULT_HEADERS.copy()
        headers["User-Agent"] = random.choice(USER_AGENTS)
        return headers

    def fetch(self) -> Optional[Union[str, dict]]:
        for attempt in range(MAX_RETRIES):
            try:
                if "/api/" in self.url or self.url.endswith(".json"):
                    resp = self.session.get(self.url, headers=self._headers(), timeout=10)
                    resp.raise_for_status()
                    return resp.json()
                else:
                    resp = self.session.get(self.url, headers=self._headers(), timeout=10)
                    resp.raise_for_status()
                    return resp.text
            except Exception as e:
                logging.warning(f"[{self.url}] ç¬¬ {attempt + 1} æ¬¡è¯·æ±‚å¤±è´¥ï¼š{e}")
                time.sleep(random.uniform(1, 2))
        return None


# ---------------- è¾…åŠ©å‡½æ•° ----------------

def safe_float(text: str) -> Optional[float]:
    try:
        return float(
            re.sub(r"[^\d.]", "", text.replace(",", "").replace("Â¥", ""))
        )
    except Exception as e:
        logging.debug(f"safe_float è§£æå¤±è´¥ï¼š{text} -> {e}")
        return None


def normalize_model_name(raw_name: str) -> str:
    raw_name = raw_name.lower().strip()
    patterns = [
        (r"iphone\s*15\s*pro\s*max", "iPhone 15 Pro Max"),
        (r"iphone\s*15\s*pro", "iPhone 15 Pro"),
        (r"iphone\s*15", "iPhone 15"),
        (r"iphone\s*14\s*pro\s*max", "iPhone 14 Pro Max"),
        (r"iphone\s*14\s*pro", "iPhone 14 Pro"),
        (r"iphone\s*14", "iPhone 14"),
        (r"iphone\s*13", "iPhone 13"),
        (r"iphone\s*12", "iPhone 12"),
        (r"iphone\s*11", "iPhone 11"),
    ]
    for pattern, name in patterns:
        if re.search(pattern, raw_name):
            return name
    return raw_name.title()


# ---------------- ä»·æ ¼è§£æå™¨ ----------------

class PriceParser:
    def __init__(self, raw: Union[str, dict], url: str):
        self.raw = raw
        self.url = url
        self.parsed = None
        self.model = "Unknown"
        self.price = None

    def _parse_html(self, html: str):
        soup = BeautifulSoup(html, "html.parser")

        # å¤šä¸ªç»“æ„å°è¯•ï¼ˆæ¨¡æ‹Ÿå…¼å®¹å¤šä¸ªç«™ç‚¹ç»“æ„ï¼‰
        candidates = [
            {"model": lambda s: s.find("h1"), "price": lambda s: s.find("span", class_="price")},
            {"model": lambda s: s.find("div", class_="product-title"), "price": lambda s: s.find("div", class_="product-price")},
            {"model": lambda s: s.select_one("meta[property='og:title']"), "price": lambda s: s.select_one("meta[itemprop='price']")},
        ]

        for c in candidates:
            try:
                model_tag = c["model"](soup)
                price_tag = c["price"](soup)

                if model_tag and price_tag:
                    model = model_tag.get("content") if model_tag.has_attr("content") else model_tag.get_text(strip=True)
                    price = price_tag.get("content") if price_tag.has_attr("content") else price_tag.get_text(strip=True)

                    model = normalize_model_name(model)
                    price = safe_float(price)

                    if model and price:
                        self.model = model
                        self.price = price
                        return
            except Exception as e:
                continue  # å°è¯•ä¸‹ä¸€ä¸ªç»“æ„

    def _parse_json(self, data: dict):
        try:
            model = data.get("model") or data.get("title") or "unknown"
            price = data.get("price") or data.get("value")

            model = normalize_model_name(model)
            price = safe_float(str(price))

            if model and price:
                self.model = model
                self.price = price
        except Exception as e:
            logging.warning(f"JSONè§£æå¤±è´¥: {self.url} -> {e}")

    def extract(self) -> Optional[Dict]:
        if isinstance(self.raw, str):
            self._parse_html(self.raw)
        elif isinstance(self.raw, dict):
            self._parse_json(self.raw)

        if self.price:
            return {
                "model": self.model,
                "price": self.price,
                "source_url": self.url,
                "timestamp": datetime.utcnow().isoformat()
            }

        logging.warning(f"æå–å¤±è´¥: {self.url}")
        return None

# ---------------- æ•°æ®åº“å­˜å‚¨ ----------------

class PriceDatabase:
    def __init__(self, db_path: str = DB_PATH):
        self.db_path = db_path
        self.conn = None
        self._connect()
        self._init_schema()

    def _connect(self):
        try:
            self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
            logging.info(f"æ•°æ®åº“è¿æ¥æˆåŠŸï¼š{self.db_path}")
        except Exception as e:
            logging.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š{e}")
            raise

    def _init_schema(self):
        try:
            query = """
            CREATE TABLE IF NOT EXISTS prices (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model TEXT,
                price REAL,
                source_url TEXT,
                timestamp TEXT,
                UNIQUE(model, price, source_url, timestamp)
            );
            """
            self.conn.execute(query)
            self.conn.commit()
            logging.info("æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logging.error(f"æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            raise

    def insert_price(self, record: Dict) -> bool:
        if not record:
            return False
        try:
            query = """
            INSERT OR IGNORE INTO prices (model, price, source_url, timestamp)
            VALUES (?, ?, ?, ?);
            """
            self.conn.execute(
                query,
                (
                    record["model"],
                    record["price"],
                    record["source_url"],
                    record["timestamp"],
                ),
            )
            self.conn.commit()
            logging.info(f"âœ… æ’å…¥æˆåŠŸ: {record['model']} - {record['price']}")
            return True
        except Exception as e:
            logging.error(f"âŒ æ’å…¥å¤±è´¥: {e} | æ•°æ®: {record}")
            return False

    def fetch_latest(self, model: str) -> Optional[float]:
        try:
            query = """
            SELECT price FROM prices
            WHERE model = ?
            ORDER BY timestamp DESC
            LIMIT 1;
            """
            cursor = self.conn.execute(query, (model,))
            row = cursor.fetchone()
            if row:
                return row[0]
            return None
        except Exception as e:
            logging.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return None

    def close(self):
        if self.conn:
            self.conn.close()
            logging.info("æ•°æ®åº“è¿æ¥å…³é—­")


# ---------------- ä¸»æ‰§è¡Œæµç¨‹ ----------------

def run_all():
    logging.info("ğŸš€ å¯åŠ¨æŠ“å–ä»»åŠ¡")

    db = PriceDatabase()

    success_count = 0
    fail_count = 0

    for idx, url in enumerate(URLS):
        logging.info(f"[{idx+1}/{len(URLS)}] æ­£åœ¨æŠ“å–: {url}")
        fetcher = PageFetcher(url)
        raw_data = fetcher.fetch()

        if not raw_data:
            logging.warning(f"âš ï¸ è·³è¿‡ç©ºå†…å®¹: {url}")
            fail_count += 1
            continue

        parser = PriceParser(raw_data, url)
        result = parser.extract()

        if not result:
            logging.warning(f"âš ï¸ è§£æå¤±è´¥: {url}")
            fail_count += 1
            continue

        inserted = db.insert_price(result)
        if inserted:
            success_count += 1
        else:
            fail_count += 1

        # é˜²æ­¢è¯·æ±‚è¿‡å¿«ï¼š1.5 åˆ° 3 ç§’ä¹‹é—´çš„éšæœºå»¶è¿Ÿ
        sleep_duration = round(random.uniform(1.5, 3.0), 2)
        logging.info(f"ç­‰å¾… {sleep_duration} ç§’ä»¥é˜²å°é”")
        time.sleep(sleep_duration)

    db.close()
    logging.info(f"âœ… æŠ“å–ä»»åŠ¡å®Œæˆã€‚æˆåŠŸ: {success_count} æ¡ï¼Œå¤±è´¥: {fail_count} æ¡")


# ---------------- é™„åŠ åŠŸèƒ½æ¨¡å— ----------------

def detect_price_drop(model: str, new_price: float, db: PriceDatabase) -> Optional[str]:
    """æ£€æŸ¥æ˜¯å¦å‡ºç°æ˜æ˜¾ä»·æ ¼ä¸‹é™ï¼Œè¿”å›è­¦å‘Šä¿¡æ¯"""
    latest = db.fetch_latest(model)
    if latest is not None and new_price < latest * 0.9:  # ä¸‹è·Œè¶…10%
        drop_percent = round((latest - new_price) / latest * 100, 2)
        message = f"âš ï¸ æ£€æµ‹åˆ°ä»·æ ¼ä¸‹è·Œï¼š{model} ä» {latest} âœ {new_price}ï¼ˆä¸‹é™ {drop_percent}%ï¼‰"
        logging.warning(message)
        return message
    return None


# ---------------- å¯é€‰å¹¶å‘ç»“æ„é¢„ç•™ ----------------

class TaskRunner:
    def __init__(self, urls: List[str]):
        self.urls = urls
        self.db = PriceDatabase()
        self.stats = {
            "success": 0,
            "fail": 0,
            "start_time": time.time()
        }

    def run_single(self, url: str, index: int):
        logging.info(f"[{index+1}] å¼€å§‹æŠ“å–ä»»åŠ¡: {url}")
        fetcher = PageFetcher(url)
        raw_data = fetcher.fetch()

        if not raw_data:
            self.stats["fail"] += 1
            return

        parser = PriceParser(raw_data, url)
        result = parser.extract()
        if not result:
            self.stats["fail"] += 1
            return

        self.db.insert_price(result)
        drop_notice = detect_price_drop(result["model"], result["price"], self.db)
        if drop_notice:
            print(drop_notice)

        self.stats["success"] += 1
        time.sleep(random.uniform(1.2, 2.5))

    def run_all_sequential(self):
        for idx, url in enumerate(self.urls):
            self.run_single(url, idx)

    def finish(self):
        self.db.close()
        elapsed = round(time.time() - self.stats["start_time"], 2)
        print(f"\nğŸ“Š æŠ¥å‘Šï¼šæˆåŠŸ {self.stats['success']} æ¡ï¼Œå¤±è´¥ {self.stats['fail']} æ¡")
        print(f"â±ï¸ æ€»è€—æ—¶ï¼š{elapsed} ç§’")
        logging.info(f"ä»»åŠ¡å®Œæˆï¼šæˆåŠŸ {self.stats['success']}ï¼Œå¤±è´¥ {self.stats['fail']}ï¼Œè€—æ—¶ {elapsed} ç§’")



if __name__ == "__main__":
    try:
        runner = TaskRunner(URLS)
        runner.run_all_sequential()
        runner.finish()
    except Exception as e:
        logging.critical(f"æœªæ•è·å¼‚å¸¸ç»ˆæ­¢ç¨‹åºï¼š{e}", exc_info=True)
        print("âŒ ç¨‹åºå¼‚å¸¸ä¸­æ­¢ï¼Œè¯·æŸ¥çœ‹ logs/get_prices.log æ—¥å¿—æ–‡ä»¶ã€‚")
